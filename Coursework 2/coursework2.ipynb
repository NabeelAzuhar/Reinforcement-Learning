{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DQN, ReplayBuffer, greedy_action, epsilon_greedy, update_target, loss\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RUNS = 10\n",
    "NUM_EPISODES = 300\n",
    "\n",
    "epsilon_start = 1\n",
    "epsilon_decay = 0.992\n",
    "\n",
    "alpha = 0.001\n",
    "\n",
    "architecture = [4, 64, 64, 2]\n",
    "batch_size = 32\n",
    "buffer_size = 20000\n",
    "target_net_update_freq = 4\n",
    "optimizer_update_freq = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_results = []\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Running the algorithm for NUM_RUNS times\n",
    "for run in range(NUM_RUNS):\n",
    "    \n",
    "    print(f\"Starting run {run+1} of {NUM_RUNS}\")\n",
    "    \n",
    "    # Initialize the policy and target network\n",
    "    policy_net = DQN(architecture)\n",
    "    target_net = DQN(architecture)\n",
    "    update_target(target_net, policy_net)\n",
    "    target_net.eval()\n",
    "\n",
    "    # Initialize epsilon, optimizer and memory\n",
    "    epsilon = epsilon_start\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=alpha)\n",
    "    memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "    # Initialize the counters\n",
    "    steps_done = 0\n",
    "    episode_durations = []\n",
    "\n",
    "    # Running the episodes\n",
    "    for i_episode in range(NUM_EPISODES):\n",
    "        \n",
    "        if (i_episode+1) % 50 == 0:\n",
    "            print(\"episode \", i_episode+1, \"/\", 300)\n",
    "\n",
    "        # Reset the environment\n",
    "        observation, info = env.reset()\n",
    "        state = torch.tensor(observation).float()\n",
    "        done = False\n",
    "        terminated = False\n",
    "        t = 0\n",
    "\n",
    "        # Running the steps\n",
    "        while not (done or terminated):\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = epsilon_greedy(epsilon, policy_net, state)\n",
    "\n",
    "            # Perform the action and observe new state and reward\n",
    "            observation, reward, done, terminated, info = env.step(action)\n",
    "            reward = torch.tensor([reward])\n",
    "            action = torch.tensor([action])\n",
    "            next_state = torch.tensor(observation).reshape(-1).float()\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push([state, action, next_state, reward, torch.tensor([done])])\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            if len(memory.buffer) >= batch_size and steps_done % optimizer_update_freq == 0:\n",
    "                transitions = memory.sample(batch_size)\n",
    "                state_batch, action_batch, nextstate_batch, reward_batch, dones = (torch.stack(x) for x in zip(*transitions))\n",
    "                # Compute loss\n",
    "                mse_loss = loss(policy_net, target_net, state_batch, action_batch, reward_batch, nextstate_batch, dones)\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                mse_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Update the step counter\n",
    "            if done or terminated:\n",
    "                episode_durations.append(t + 1)\n",
    "            t += 1\n",
    "            steps_done += 1\n",
    "\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if i_episode % target_net_update_freq == 0: \n",
    "            update_target(target_net, policy_net)\n",
    "\n",
    "        # Update epsilon\n",
    "        epsilon *= epsilon_decay\n",
    "    \n",
    "    # Append the results of the run\n",
    "    runs_results.append(episode_durations)\n",
    "    \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the learning curve\n",
    "# Placeholder plot, you are free to modify it\n",
    " \n",
    "results = torch.tensor(runs_results)\n",
    "means = results.float().mean(0)\n",
    "stds = results.float().std(0)\n",
    "average = means[-101:-1].mean()\n",
    "print(f'Average reward over the last 100 episodes: {average}')\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(torch.arange(NUM_EPISODES), means, linewidth=0.5, label='Mean')\n",
    "plt.title(\"Learning Curve of the DQN agent\", fontsize=14)\n",
    "plt.ylabel(\"Return\", fontsize=12)\n",
    "plt.xlabel(\"Episode\", fontsize=12)\n",
    "plt.fill_between(np.arange(NUM_EPISODES), means, means+stds, alpha=0.3, color='b', label=\"Standard Deviation\")\n",
    "plt.fill_between(np.arange(NUM_EPISODES), means, means-stds, alpha=0.3, color='b')\n",
    "plt.axhline(y=100, color='r', linestyle='--', label=\"Return threshold\", linewidth=0.8)\n",
    "plt.axhline(y=average, color='g', linestyle='--', label=f\"Mean final returns\", linewidth=0.8)\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the greedy Q-values for a stationary cart in the middle of the track\n",
    "# 2D plot showing policy as a function of pole angle and angular velocity (omega)\n",
    "\n",
    "# This plots the policy and Q values according to the network currently\n",
    "# stored in the variable \"policy_net\"\n",
    "\n",
    "# All visualisations provided here are placeholders and you can modify these plots\n",
    "\n",
    "# Make sure to include appropriate labels and/or legends when presenting your plot\n",
    "\n",
    "# policy_net = DQN([4,2])   # randomly initialised, replace with your trained DQN\n",
    "# q = True    # whether q values or greedy policy is visualised\n",
    "\n",
    "angle_range = .2095 # you may modify this range\n",
    "omega_range = 1     # you may modify this range\n",
    "\n",
    "angle_samples = 100\n",
    "omega_samples = 100\n",
    "angles = torch.linspace(angle_range, -angle_range, angle_samples)\n",
    "omegas = torch.linspace(-omega_range, omega_range, omega_samples)\n",
    "velocities = [0, 0.5, 1, 2]\n",
    "\n",
    "greedy_q_array = torch.zeros((angle_samples, omega_samples))\n",
    "policy_array = torch.zeros((angle_samples, omega_samples))\n",
    "\n",
    "for velocity in velocities:\n",
    "    for i, angle in enumerate(angles):\n",
    "        for j, omega in enumerate(omegas):\n",
    "            state = torch.tensor([0., velocity, angle, omega])\n",
    "            with torch.no_grad():\n",
    "                q_vals = policy_net(state)\n",
    "                greedy_action = q_vals.argmax()\n",
    "                greedy_q_array[i, j] = q_vals[greedy_action]\n",
    "                policy_array[i, j] = greedy_action\n",
    "    \n",
    "    # Plotting the greedy policy\n",
    "    plt.contourf(angles, omegas, policy_array.T, cmap='cividis')\n",
    "    # Creating custom legend patches\n",
    "    left_patch = mpatches.Patch(color='blue', label='Action 0')\n",
    "    right_patch = mpatches.Patch(color='yellow', label='Action 1')\n",
    "    plt.title(f\"Policy for cart velocity = {velocity}\")\n",
    "    plt.xlabel(\"Pole angle\")\n",
    "    plt.ylabel(\"Pole angular velocity\")\n",
    "    plt.legend(handles=[left_patch, right_patch], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plotting the greedy Q-values\n",
    "    contour_q = plt.contourf(angles, omegas, greedy_q_array.T, cmap='cividis', levels=100)\n",
    "    # Adding a colorbar as a legend\n",
    "    colorbar = plt.colorbar(contour_q)\n",
    "    colorbar.set_label('Magnitude')\n",
    "    plt.title(f\"Q-values for cart velocity = {velocity}\")\n",
    "    plt.xlabel(\"Pole angle\")\n",
    "    plt.ylabel(\"Pole angular velocity\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('prettygym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19a620d5ac44fc7d0064c056f791ee6a0d71d7061288207e2ea49537a4625f93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
