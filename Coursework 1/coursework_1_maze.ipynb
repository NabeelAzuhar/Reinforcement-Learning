{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1568,
      "metadata": {
        "id": "20IyxDzgp3tU"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import random\n",
        "import matplotlib.pyplot as plt # Graphical library\n",
        "#from sklearn.metrics import mean_squared_error # Mean-squared error function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2Fr69C0UBQk"
      },
      "source": [
        "# Coursework 1 :\n",
        "See pdf for instructions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1569,
      "metadata": {
        "id": "QsKvVllvvreH"
      },
      "outputs": [],
      "source": [
        "# WARNING: fill in these two functions that will be used by the auto-marking script\n",
        "# [Action required]\n",
        "\n",
        "def get_CID():\n",
        "  return \"01701146\" # Return your CID (add 0 at the beginning to ensure it is 8 digits long)\n",
        "\n",
        "def get_login():\n",
        "  return \"na2919\" # Return your short imperial login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKEz3d9NUbdO"
      },
      "source": [
        "## Helper class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1570,
      "metadata": {
        "id": "ZWnMW3GNpjd7"
      },
      "outputs": [],
      "source": [
        "# This class is used ONLY for graphics\n",
        "# YOU DO NOT NEED to understand it to work on this coursework\n",
        "\n",
        "class GraphicsMaze(object):\n",
        "\n",
        "  def __init__(self, shape, locations, default_reward, obstacle_locs, absorbing_locs, absorbing_rewards, absorbing):\n",
        "\n",
        "    self.shape = shape\n",
        "    self.locations = locations\n",
        "    self.absorbing = absorbing\n",
        "\n",
        "    # Walls\n",
        "    self.walls = np.zeros(self.shape)\n",
        "    for ob in obstacle_locs:\n",
        "      self.walls[ob] = 20\n",
        "\n",
        "    # Rewards\n",
        "    self.rewarders = np.ones(self.shape) * default_reward\n",
        "    for i, rew in enumerate(absorbing_locs):\n",
        "      self.rewarders[rew] = 10 if absorbing_rewards[i] > 0 else -10\n",
        "\n",
        "    # Print the map to show it\n",
        "    self.paint_maps()\n",
        "\n",
        "  def paint_maps(self):\n",
        "    \"\"\"\n",
        "    Print the Maze topology (obstacles, absorbing states and rewards)\n",
        "    input: /\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.imshow(self.walls + self.rewarders)\n",
        "    plt.show()\n",
        "\n",
        "  def paint_state(self, state):\n",
        "    \"\"\"\n",
        "    Print one state on the Maze topology (obstacles, absorbing states and rewards)\n",
        "    input: /\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    states = np.zeros(self.shape)\n",
        "    states[state] = 30\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.imshow(self.walls + self.rewarders + states)\n",
        "    plt.show()\n",
        "\n",
        "  def draw_deterministic_policy(self, Policy):\n",
        "    \"\"\"\n",
        "    Draw a deterministic policy\n",
        "    input: Policy {np.array} -- policy to draw (should be an array of values between 0 and 3 (actions))\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze\n",
        "    for state, action in enumerate(Policy):\n",
        "      if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action\n",
        "        continue\n",
        "      arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] # List of arrows corresponding to each possible action\n",
        "      action_arrow = arrows[action] # Take the corresponding action\n",
        "      location = self.locations[state] # Compute its location on graph\n",
        "      plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph\n",
        "    plt.show()\n",
        "\n",
        "  def draw_policy(self, Policy):\n",
        "    \"\"\"\n",
        "    Draw a policy (draw an arrow in the most probable direction)\n",
        "    input: Policy {np.array} -- policy to draw as probability\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    deterministic_policy = np.array([np.argmax(Policy[row,:]) for row in range(Policy.shape[0])])\n",
        "    self.draw_deterministic_policy(deterministic_policy)\n",
        "\n",
        "  def draw_value(self, Value):\n",
        "    \"\"\"\n",
        "    Draw a policy value\n",
        "    input: Value {np.array} -- policy values to draw\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze\n",
        "    for state, value in enumerate(Value):\n",
        "      if(self.absorbing[0, state]): # If it is an absorbing state, don't plot any value\n",
        "        continue\n",
        "      location = self.locations[state] # Compute the value location on graph\n",
        "      plt.text(location[1], location[0], round(value,2), ha='center', va='center') # Place it on graph\n",
        "    plt.show()\n",
        "\n",
        "  def draw_deterministic_policy_grid(self, Policies, title, n_columns, n_lines):\n",
        "    \"\"\"\n",
        "    Draw a grid representing multiple deterministic policies\n",
        "    input: Policies {np.array of np.array} -- array of policies to draw (each should be an array of values between 0 and 3 (actions))\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for subplot in range (len(Policies)): # Go through all policies\n",
        "      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each policy\n",
        "      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze\n",
        "      for state, action in enumerate(Policies[subplot]):\n",
        "        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action\n",
        "          continue\n",
        "        arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] # List of arrows corresponding to each possible action\n",
        "        action_arrow = arrows[action] # Take the corresponding action\n",
        "        location = self.locations[state] # Compute its location on graph\n",
        "        plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph\n",
        "      ax.title.set_text(title[subplot]) # Set the title for the graph given as argument\n",
        "    plt.show()\n",
        "\n",
        "  def draw_policy_grid(self, Policies, title, n_columns, n_lines):\n",
        "    \"\"\"\n",
        "    Draw a grid representing multiple policies (draw an arrow in the most probable direction)\n",
        "    input: Policy {np.array} -- array of policies to draw as probability\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    deterministic_policies = np.array([[np.argmax(Policy[row,:]) for row in range(Policy.shape[0])] for Policy in Policies])\n",
        "    self.draw_deterministic_policy_grid(deterministic_policies, title, n_columns, n_lines)\n",
        "\n",
        "  def draw_value_grid(self, Values, title, n_columns, n_lines):\n",
        "    \"\"\"\n",
        "    Draw a grid representing multiple policy values\n",
        "    input: Values {np.array of np.array} -- array of policy values to draw\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for subplot in range (len(Values)): # Go through all values\n",
        "      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each value\n",
        "      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze\n",
        "      for state, value in enumerate(Values[subplot]):\n",
        "        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any value\n",
        "          continue\n",
        "        location = self.locations[state] # Compute the value location on graph\n",
        "        plt.text(location[1], location[0], round(value,1), ha='center', va='center') # Place it on graph\n",
        "      ax.title.set_text(title[subplot]) # Set the title for the graoh given as argument\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbY8DCqoVJlw"
      },
      "source": [
        "## Maze class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1571,
      "metadata": {
        "id": "MXc1OFvZqJfZ"
      },
      "outputs": [],
      "source": [
        "# This class define the Maze environment\n",
        "\n",
        "class Maze(object):\n",
        "\n",
        "  # [Action required]\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Maze initialisation.\n",
        "    input: /\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    \n",
        "    # [Action required]\n",
        "    # Properties set from the CID\n",
        "    self.y = 9\n",
        "    self.z = 9\n",
        "    self._prob_success = 0.8 + 0.02 * (9 - self.y) # float\n",
        "    self._gamma = 0.8 + 0.02 * self.y # float\n",
        "    self._goal = int(self.z % 4) # integer (0 for R0, 1 for R1, 2 for R2, 3 for R3)\n",
        "\n",
        "    # Build the maze\n",
        "    self._build_maze()\n",
        "                              \n",
        "\n",
        "  # Functions used to build the Maze environment \n",
        "  # You DO NOT NEED to modify them\n",
        "  def _build_maze(self):\n",
        "    \"\"\"\n",
        "    Maze initialisation.\n",
        "    input: /\n",
        "    output: /\n",
        "    \"\"\"\n",
        "\n",
        "    # Properties of the maze\n",
        "    self._shape = (13, 10)\n",
        "    self._obstacle_locs = [\n",
        "                          (1,0), (1,1), (1,2), (1,3), (1,4), (1,7), (1,8), (1,9), \\\n",
        "                          (2,1), (2,2), (2,3), (2,7), \\\n",
        "                          (3,1), (3,2), (3,3), (3,7), \\\n",
        "                          (4,1), (4,7), \\\n",
        "                          (5,1), (5,7), \\\n",
        "                          (6,5), (6,6), (6,7), \\\n",
        "                          (8,0), \\\n",
        "                          (9,0), (9,1), (9,2), (9,6), (9,7), (9,8), (9,9), \\\n",
        "                          (10,0)\n",
        "                         ] # Location of obstacles\n",
        "    self._absorbing_locs = [(2,0), (2,9), (10,1), (12,9)] # Location of absorbing states\n",
        "    self._absorbing_rewards = [ (500 if (i == self._goal) else -50) for i in range (4) ]\n",
        "    self._starting_locs = [(0,0), (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7), (0,8), (0,9)] #Reward of absorbing states\n",
        "    self._default_reward = -1 # Reward for each action performs in the environment\n",
        "    self._max_t = 500 # Max number of steps in the environment\n",
        "\n",
        "    # Actions\n",
        "    self._action_size = 4\n",
        "    self._direction_names = ['N','E','S','W'] # Direction 0 is 'N', 1 is 'E' and so on\n",
        "        \n",
        "    # States\n",
        "    self._locations = []\n",
        "    for i in range (self._shape[0]):\n",
        "      for j in range (self._shape[1]):\n",
        "        loc = (i,j) \n",
        "        # Adding the state to locations if it is no obstacle\n",
        "        if self._is_location(loc):\n",
        "          self._locations.append(loc)\n",
        "    self._state_size = len(self._locations)\n",
        "\n",
        "    # Neighbours - each line is a state, ranked by state-number, each column is a direction (N, E, S, W)\n",
        "    self._neighbours = np.zeros((self._state_size, 4)) \n",
        "    \n",
        "    for state in range(self._state_size):\n",
        "      loc = self._get_loc_from_state(state)\n",
        "\n",
        "      # North\n",
        "      neighbour = (loc[0]-1, loc[1]) # North neighbours location\n",
        "      if self._is_location(neighbour):\n",
        "        self._neighbours[state][self._direction_names.index('N')] = self._get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        self._neighbours[state][self._direction_names.index('N')] = state\n",
        "\n",
        "      # East\n",
        "      neighbour = (loc[0], loc[1]+1) # East neighbours location\n",
        "      if self._is_location(neighbour):\n",
        "        self._neighbours[state][self._direction_names.index('E')] = self._get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        self._neighbours[state][self._direction_names.index('E')] = state\n",
        "\n",
        "      # South\n",
        "      neighbour = (loc[0]+1, loc[1]) # South neighbours location\n",
        "      if self._is_location(neighbour):\n",
        "        self._neighbours[state][self._direction_names.index('S')] = self._get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        self._neighbours[state][self._direction_names.index('S')] = state\n",
        "\n",
        "      # West\n",
        "      neighbour = (loc[0], loc[1]-1) # West neighbours location\n",
        "      if self._is_location(neighbour):\n",
        "        self._neighbours[state][self._direction_names.index('W')] = self._get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        self._neighbours[state][self._direction_names.index('W')] = state\n",
        "\n",
        "    # Absorbing\n",
        "    self._absorbing = np.zeros((1, self._state_size))\n",
        "    for a in self._absorbing_locs:\n",
        "      absorbing_state = self._get_state_from_loc(a)\n",
        "      self._absorbing[0, absorbing_state] = 1\n",
        "\n",
        "    # Transition matrix\n",
        "    self._T = np.zeros((self._state_size, self._state_size, self._action_size)) # Empty matrix of domension S*S*A\n",
        "    for action in range(self._action_size):\n",
        "      for outcome in range(4): # For each direction (N, E, S, W)\n",
        "        # The agent has prob_success probability to go in the correct direction\n",
        "        if action == outcome:\n",
        "          prob = 1 - 3.0 * ((1.0 - self._prob_success) / 3.0) # (theoritically equal to self.prob_success but avoid rounding error and garanty a sum of 1)\n",
        "        # Equal probability to go into one of the other directions\n",
        "        else:\n",
        "          prob = (1.0 - self._prob_success) / 3.0\n",
        "          \n",
        "        # Write this probability in the transition matrix\n",
        "        for prior_state in range(self._state_size):\n",
        "          # If absorbing state, probability of 0 to go to any other states\n",
        "          if not self._absorbing[0, prior_state]:\n",
        "            post_state = self._neighbours[prior_state, outcome] # Post state number\n",
        "            post_state = int(post_state) # Transform in integer to avoid error\n",
        "            self._T[prior_state, post_state, action] += prob\n",
        "\n",
        "    # Reward matrix\n",
        "    self._R = np.ones((self._state_size, self._state_size, self._action_size)) # Matrix filled with 1\n",
        "    self._R = self._default_reward * self._R # Set default_reward everywhere\n",
        "    for i in range(len(self._absorbing_rewards)): # Set absorbing states rewards\n",
        "      post_state = self._get_state_from_loc(self._absorbing_locs[i])\n",
        "      self._R[:,post_state,:] = self._absorbing_rewards[i]\n",
        "\n",
        "    # Creating the graphical Maze world\n",
        "    self._graphics = GraphicsMaze(self._shape, self._locations, self._default_reward, self._obstacle_locs, self._absorbing_locs, self._absorbing_rewards, self._absorbing)\n",
        "    \n",
        "    # Reset the environment\n",
        "    self.reset()\n",
        "\n",
        "\n",
        "  def _is_location(self, loc):\n",
        "    \"\"\"\n",
        "    Is the location a valid state (not out of Maze and not an obstacle)\n",
        "    input: loc {tuple} -- location of the state\n",
        "    output: _ {bool} -- is the location a valid state\n",
        "    \"\"\"\n",
        "    if (loc[0] < 0 or loc[1] < 0 or loc[0] > self._shape[0]-1 or loc[1] > self._shape[1]-1):\n",
        "      return False\n",
        "    elif (loc in self._obstacle_locs):\n",
        "      return False\n",
        "    else:\n",
        "      return True\n",
        "\n",
        "\n",
        "  def _get_state_from_loc(self, loc):\n",
        "    \"\"\"\n",
        "    Get the state number corresponding to a given location\n",
        "    input: loc {tuple} -- location of the state\n",
        "    output: index {int} -- corresponding state number\n",
        "    \"\"\"\n",
        "    return self._locations.index(tuple(loc))\n",
        "\n",
        "\n",
        "  def _get_loc_from_state(self, state):\n",
        "    \"\"\"\n",
        "    Get the state number corresponding to a given location\n",
        "    input: index {int} -- state number\n",
        "    output: loc {tuple} -- corresponding location\n",
        "    \"\"\"\n",
        "    return self._locations[state]\n",
        "\n",
        "  # Getter functions used only for DP agents\n",
        "  # You DO NOT NEED to modify them\n",
        "  def get_T(self):\n",
        "    return self._T\n",
        "\n",
        "  def get_R(self):\n",
        "    return self._R\n",
        "\n",
        "  def get_absorbing(self):\n",
        "    return self._absorbing\n",
        "\n",
        "  # Getter functions used for DP, MC and TD agents\n",
        "  # You DO NOT NEED to modify them\n",
        "  def get_graphics(self):\n",
        "    return self._graphics\n",
        "\n",
        "  def get_action_size(self):\n",
        "    return self._action_size\n",
        "\n",
        "  def get_state_size(self):\n",
        "    return self._state_size\n",
        "\n",
        "  def get_gamma(self):\n",
        "    return self._gamma\n",
        "\n",
        "  # Functions used to perform episodes in the Maze environment\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment state to one of the possible starting states\n",
        "    input: /\n",
        "    output: \n",
        "      - t {int} -- current timestep\n",
        "      - state {int} -- current state of the envionment\n",
        "      - reward {int} -- current reward\n",
        "      - done {bool} -- True if reach a terminal state / 0 otherwise\n",
        "    \"\"\"\n",
        "    self._t = 0\n",
        "    self._state = self._get_state_from_loc(self._starting_locs[random.randrange(len(self._starting_locs))])\n",
        "    self._reward = 0\n",
        "    self._done = False\n",
        "    return self._t, self._state, self._reward, self._done\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Perform an action in the environment\n",
        "    input: action {int} -- action to perform\n",
        "    output: \n",
        "      - t {int} -- current timestep\n",
        "      - state {int} -- current state of the envionment\n",
        "      - reward {int} -- current reward\n",
        "      - done {bool} -- True if reach a terminal state / 0 otherwise\n",
        "    \"\"\"\n",
        "\n",
        "    # If environment already finished, print an error\n",
        "    if self._done or self._absorbing[0, self._state]:\n",
        "      print(\"Please reset the environment\")\n",
        "      return self._t, self._state, self._reward, self._done\n",
        "\n",
        "    # Drawing a random number used for probaility of next state\n",
        "    probability_success = random.uniform(0,1)\n",
        "\n",
        "    # Look for the first possible next states (so get a reachable state even if probability_success = 0)\n",
        "    new_state = 0\n",
        "    while self._T[self._state, new_state, action] == 0: \n",
        "      new_state += 1\n",
        "    assert self._T[self._state, new_state, action] != 0, \"Selected initial state should be probability 0, something might be wrong in the environment.\"\n",
        "\n",
        "    # Find the first state for which probability of occurence matches the random value\n",
        "    total_probability = self._T[self._state, new_state, action]\n",
        "    while (total_probability < probability_success) and (new_state < self._state_size-1):\n",
        "     new_state += 1\n",
        "     total_probability += self._T[self._state, new_state, action]\n",
        "    assert self._T[self._state, new_state, action] != 0, \"Selected state should be probability 0, something might be wrong in the environment.\"\n",
        "    \n",
        "    # Setting new t, state, reward and done\n",
        "    self._t += 1\n",
        "    self._reward = self._R[self._state, new_state, action]\n",
        "    self._done = self._absorbing[0, new_state] or self._t > self._max_t\n",
        "    self._state = new_state\n",
        "    return self._t, self._state, self._reward, self._done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW3Ul0q-VRE-"
      },
      "source": [
        "## DP Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1572,
      "metadata": {
        "id": "3ucYXx5NqStY"
      },
      "outputs": [],
      "source": [
        "# This class define the Dynamic Programing agent \n",
        "\n",
        "class DP_agent(object):\n",
        "\n",
        "  # [Action required]\n",
        "  # WARNING: make sure this function can be called by the auto-marking script\n",
        "  def solve(self, env):\n",
        "    \"\"\"\n",
        "    Solve a given Maze environment using Dynamic Programming\n",
        "    input: env {Maze object} -- Maze to solve\n",
        "    output: \n",
        "      - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
        "      - V {np.array} -- Corresponding value function \n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialisation (can be edited)\n",
        "    policy = np.zeros((env.get_state_size(), env.get_action_size())) \n",
        "    V = np.zeros(env.get_state_size())\n",
        "    threshold = 0.1\n",
        "    delta = threshold\n",
        "    T = env.get_T()\n",
        "    R = env.get_R()\n",
        "    gamma = env.get_gamma()\n",
        "\n",
        "    #### \n",
        "    # Add your code here\n",
        "    # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()\n",
        "    ####\n",
        "\n",
        "    # Value iteration\n",
        "    # Compute optimal value function\n",
        "    epoch = 0\n",
        "    while delta >= threshold:\n",
        "      delta = 0\n",
        "      delta_state = 0\n",
        "      for state in range(env.get_state_size()):\n",
        "        # If not absorbing state\n",
        "        if not env.get_absorbing()[0, state]:\n",
        "          # Store previous state value\n",
        "          v = V[state]\n",
        "          # Compute new state value\n",
        "          Q = np.zeros(4)\n",
        "          for next_state in range(env.get_state_size()):\n",
        "            Q += T[state, next_state, :] * (R[state, next_state, :] + gamma * V[next_state])\n",
        "          V[state] = np.max(Q)\n",
        "          # Update delta\n",
        "          delta = max(delta, abs(v - V[state]))\n",
        "      epoch += 1\n",
        "    print('Number of epochs: ', epoch)\n",
        "    \n",
        "    # Compute optimal policy\n",
        "    for state in range(env.get_state_size()):\n",
        "      # If not absorbing state\n",
        "      if not env.get_absorbing()[0, state]:\n",
        "        # Compute Q values\n",
        "        Q = np.zeros(4)\n",
        "        for next_state in range(env.get_state_size()):\n",
        "          Q += T[state, next_state, :] * (R[state, next_state, :] + gamma * V[next_state])\n",
        "        # Compute optimal policy\n",
        "        policy[state, np.argmax(Q)] = 1\n",
        "\n",
        "    return policy, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14i0zRkdVSqk"
      },
      "source": [
        "## MC agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1573,
      "metadata": {
        "id": "CdHvHvcSrEW9"
      },
      "outputs": [],
      "source": [
        "# # This class define the Monte-Carlo agent\n",
        "\n",
        "# class MC_agent(object):\n",
        "  \n",
        "#   def learning_curve(self, env):\n",
        "#     learning_rewards = []\n",
        "#     for i in range(25):\n",
        "#       print('Run ', i)\n",
        "#       _, _, total_rewards = self.solve(env)\n",
        "#       learning_rewards.append(total_rewards)\n",
        "#     learning_rewards = np.array(learning_rewards)\n",
        "#     mean_rewards = np.mean(learning_rewards, axis = 0)\n",
        "#     std_rewards = np.std(learning_rewards, axis = 0)\n",
        "#     # plot the mean and standard deviation of the rewards on the same graph\n",
        "#     plt.figure(figsize=(15,10))\n",
        "#     plt.plot(mean_rewards, label = 'Mean', color = 'b', linewidth = 2)\n",
        "#     plt.fill_between(range(len(mean_rewards)), mean_rewards + std_rewards, mean_rewards - std_rewards, alpha = 0.2, color = 'skyblue')\n",
        "#     plt.title('Learning Curve for Monte Carlo Agent across 25 simulations', fontsize = 14)\n",
        "#     plt.xlabel('Episode Number', fontsize = 12)\n",
        "#     plt.ylabel('Total Reward', fontsize = 12)\n",
        "#     legend_elements = [plt.Line2D([0], [0], color='b', label='Mean Rewards', linewidth = 2),\n",
        "#                       plt.Rectangle((0, 0), 1, 1, fc=\"skyblue\", alpha=0.3, edgecolor='none', label='Standard Deviation')]\n",
        "#     plt.legend(handles=legend_elements, fontsize = 10, loc = 'lower right')\n",
        "#     plt.grid(alpha = 0.3)\n",
        "#     plt.show()\n",
        "#     print('Last mean reward', learning_rewards[:,-5:-1])\n",
        "\n",
        "  \n",
        "#   # [Action required]\n",
        "#   # WARNING: make sure this function can be called by the auto-marking script\n",
        "#   def solve(self, env):\n",
        "#     \"\"\"\n",
        "#     Solve a given Maze environment using Monte Carlo learning\n",
        "#     input: env {Maze object} -- Maze to solve\n",
        "#     output: \n",
        "#       - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
        "#       - values {list of np.array} -- List of successive value functions for each episode \n",
        "#       - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode \n",
        "#     \"\"\"\n",
        "\n",
        "#     def epsilon_greedy(policy, epsilon):\n",
        "#       \"\"\"\n",
        "#       Compute an epsilon-greedy policy from a given policy\n",
        "#       input: policy {np.array} -- policy to convert\n",
        "#       output: epsilon_greedy_policy {np.array} -- epsilon-greedy policy\n",
        "#       \"\"\"\n",
        "#       epsilon_greedy_policy = np.zeros(policy.shape)\n",
        "#       for state in range(policy.shape[0]):\n",
        "#         max_arg = np.argmax(policy[state, :])\n",
        "#         for action in range(policy.shape[1]):\n",
        "#           epsilon_greedy_policy[state, action] = epsilon / policy.shape[1]\n",
        "#         epsilon_greedy_policy[state, max_arg] = 1 - epsilon + epsilon / policy.shape[1]\n",
        "#       return epsilon_greedy_policy\n",
        "\n",
        "#     # Initialisation (can be edited)\n",
        "#     epsilon = 0.8\n",
        "#     Q = np.random.rand(env.get_state_size(), env.get_action_size()) \n",
        "#     V = np.zeros(env.get_state_size())\n",
        "#     policy = epsilon_greedy(np.zeros((env.get_state_size(), env.get_action_size())), epsilon) # Epsilon greedy policy\n",
        "#     values = [V]\n",
        "#     total_rewards = []\n",
        "#     gamma = env.get_gamma()\n",
        "#     returns = [[[] for i in range(env.get_action_size())] for j in range(env.get_state_size())]\n",
        "\n",
        "#     #### \n",
        "#     # Add your code here\n",
        "#     # WARNING: this agent only has access to env.reset() and env.step()\n",
        "#     # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value\n",
        "\n",
        "#     # On-policy first-visit MC control (for epsilon-soft policies)\n",
        "#     # Loop for each episode\n",
        "#     for episode_count in range(5000):\n",
        "#       # Reset environment\n",
        "#       t, state, reward, done = env.reset()\n",
        "#       total_reward = 0\n",
        "#       # Generate episode\n",
        "#       states = []\n",
        "#       rewards = []\n",
        "#       actions = []\n",
        "#       step_count = 0\n",
        "#       while not done and step_count < 500:\n",
        "#         step_count += 1\n",
        "#         # Choose action\n",
        "#         action = np.random.choice(env.get_action_size(), p = policy[state, :])\n",
        "#         # Perform action\n",
        "#         t, next_state, reward, done = env.step(action)\n",
        "#         # Store transition\n",
        "#         states.append(state)\n",
        "#         rewards.append(reward)\n",
        "#         actions.append(action)\n",
        "#         total_reward += reward\n",
        "#         state = next_state\n",
        "#       # Compute returns\n",
        "#       G = 0\n",
        "#       for t in reversed(range(0, len(states))):\n",
        "#         G = gamma * G + rewards[t]\n",
        "#         returns[states[t]][actions[t]].append(G)\n",
        "#         # Update Q\n",
        "#         Q[states[t], actions[t]] = np.mean(returns[states[t]][actions[t]])\n",
        "#         # Update policy\n",
        "#         best_action = np.argmax(Q[states[t], :])\n",
        "#         for action in range(env.get_action_size()):\n",
        "#           if action == best_action:\n",
        "#             policy[states[t], action] = 1 - epsilon + epsilon / env.get_action_size()\n",
        "#           else:\n",
        "#             policy[states[t], action] = epsilon / env.get_action_size()\n",
        "#       # Compute total reward\n",
        "#       total_rewards.append(total_reward)\n",
        "#       # Update values\n",
        "#       V = np.max(Q, axis = 1)\n",
        "#       values.append(V)\n",
        "#       # Update epsilon\n",
        "#       epsilon = epsilon * 0.9995\n",
        "#       # print(episode_count, epsilon)\n",
        "#     ####\n",
        "    \n",
        "#     return policy, values, total_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1578,
      "metadata": {},
      "outputs": [],
      "source": [
        "## my mc\n",
        "from matplotlib.rcsetup import ValidateInStrings\n",
        "# This class define the Monte-Carlo agent\n",
        "# check variance for every visit\n",
        "class MC_agent(object):\n",
        "\n",
        "  # [Action required]\n",
        "  # WARNING: make sure this function can be called by the auto-marking script\n",
        "  def get_e_soft_greedy_policy(self,env,epsilon,Q,state,policy):\n",
        "    best_action = np.argmax(Q[state,:])\n",
        "    policy[state,:] = epsilon/env.get_action_size()\n",
        "    policy[state,best_action] += 1-epsilon\n",
        "    return policy\n",
        "\n",
        "  def get_trace(self,env, policy,action_size):\n",
        "    time_step, state, _, done = env.reset() #reward is 0\n",
        "    trace = []\n",
        "    while done!=1: \n",
        "      rnd=random.random()\n",
        "      total_probability=0\n",
        "      for a in range(action_size):\n",
        "        total_probability+=policy[state,a]\n",
        "        if total_probability>rnd:\n",
        "          action=a\n",
        "          break\n",
        "      #action = np.random.choice(env.get_action_size(), p = policy[state, :])\n",
        "      time_step, next_state, reward, done = env.step(action)\n",
        "      trace.append((state, action, reward))\n",
        "      state = next_state\n",
        "    return trace\n",
        "\n",
        "  def solve(self, env):\n",
        "    \"\"\"\n",
        "    Solve a given Maze environment using Monte Carlo learning\n",
        "    input: env {Maze object} -- Maze to solve\n",
        "    output:\n",
        "      - policy {np.array} -- Optimal policy found to solve the given Maze environment\n",
        "      - values {list of np.array} -- List of successive value functions for each episode\n",
        "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialisation (can be edited)\n",
        "    episodes_no = 10000\n",
        "    epsilon = 0.8\n",
        "    decay_factor = 0.9995\n",
        "    Q = np.random.rand(env.get_state_size(), env.get_action_size())\n",
        "    policy = np.zeros((env.get_state_size(), env.get_action_size()))\n",
        "    V = np.zeros(env.get_state_size())\n",
        "    total_rewards = []\n",
        "    values = [V]\n",
        "    state_size = env.get_state_size()\n",
        "    action_size = env.get_action_size()\n",
        "    Returns = [[[] for _ in range(action_size)] for _ in range(state_size)]\n",
        "    for state in range(state_size):\n",
        "      policy = self.get_e_soft_greedy_policy(env,epsilon,Q,state,policy)\n",
        "\n",
        "    for episode in range(episodes_no):\n",
        "      reward_sum = 0\n",
        "      trace = self.get_trace(env,policy,action_size)\n",
        "      G = 0\n",
        "      for time_step in range(len(trace) - 1, 0, -1):\n",
        "        state,action,reward = trace[time_step]\n",
        "        reward_sum +=reward\n",
        "        G = env.get_gamma()*G + reward\n",
        "        #if not any((s == state and a == action) for s, a, _ in trace[:time_step]):\n",
        "        Returns[state][action].append(G)\n",
        "        Q[state,action] = sum(Returns[state][action])/len(Returns[state][action])\n",
        "        policy = self.get_e_soft_greedy_policy(env,epsilon,Q,state,policy)\n",
        "      #for state in range(state_size):\n",
        "      #   V[state] = np.max(Q[state,:])\n",
        "      V = np.max(Q,axis=1)\n",
        "      values.append(V)\n",
        "      total_rewards.append(reward_sum)\n",
        "      #if epsilon>0.4 and episode%10==0:\n",
        "      epsilon = epsilon*decay_factor\n",
        "\n",
        "    policy = np.zeros((env.get_state_size(), env.get_action_size()))\n",
        "    for state in range(env.get_state_size()):\n",
        "      action = np.argmax(Q[state,:])\n",
        "      policy[state, action] = 1\n",
        "    \n",
        "\n",
        "\n",
        "    return policy, values, total_rewards\n",
        "\n",
        "  def plot_learning_curve(self,env,no_trials,no_episodes):\n",
        "      rewards_list=[]\n",
        "      for trial in range(1,no_trials+1):\n",
        "        print(\"run \"+ str(trial))\n",
        "        _,_,trial_reward = self.solve(env)\n",
        "        rewards_list.append(trial_reward)\n",
        "      #episodes = [episode for episode in no_episodes]\n",
        "      rewards_array = np.array(rewards_list)\n",
        "      mean_rewards = np.mean(rewards_array,axis=0)\n",
        "      std_rewards = np.std(rewards_array,axis=0)\n",
        "      plt.plot(range(1,len(mean_rewards)+1),mean_rewards, label='Mean', color='blue')\n",
        "      plt.fill_between(range(1,len(mean_rewards)+1), mean_rewards+std_rewards,mean_rewards-std_rewards,alpha=0.4, label='Standard Deviation',color='skyblue')\n",
        "\n",
        "      # Add labels and a legend\n",
        "      plt.title('Learning Curve for Monte Carlo Agent across 25 simulations', fontsize = 14)\n",
        "      plt.xlabel('Episode number')\n",
        "      plt.ylabel('Total reward')\n",
        "      plt.grid()\n",
        "      legend_elements = [plt.Line2D([0], [0], color='b', label='Mean Rewards'),\n",
        "                      plt.Rectangle((0, 0), 1, 1, fc=\"skyblue\", alpha=0.3, edgecolor='none', label='Standard Deviation')]\n",
        "      plt.legend(handles=legend_elements, loc = 'lower right')\n",
        "\n",
        "      # Show the plot\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1575,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from matplotlib.rcsetup import ValidateInStrings\n",
        "# # This class define the Monte-Carlo agent\n",
        "# # check variance for every visit\n",
        "# class MC_agent(object):\n",
        "\n",
        "#   # [Action required]\n",
        "#   # WARNING: make sure this function can be called by the auto-marking script\n",
        "\n",
        "#   def get_e_soft_greedy_policy(self,env,epsilon,policy):\n",
        "#     soft_policy = np.zeros(policy.shape)\n",
        "#     for state in range(policy.shape[0]):\n",
        "#       best_action = np.argmax(policy[state,:])\n",
        "#       for action in range(policy.shape[1]):\n",
        "#         soft_policy[state,action] = epsilon/policy.shape[1]\n",
        "#       soft_policy[state,best_action] += 1-epsilon\n",
        "#     return soft_policy\n",
        "  \n",
        "\n",
        "#   def solve(self, env):\n",
        "#     \"\"\"\n",
        "#     Solve a given Maze environment using Monte Carlo learning\n",
        "#     input: env {Maze object} -- Maze to solve\n",
        "#     output:\n",
        "#       - policy {np.array} -- Optimal policy found to solve the given Maze environment\n",
        "#       - values {list of np.array} -- List of successive value functions for each episode\n",
        "#       - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Initialisation (can be edited)\n",
        "#     episodes_no = 5000\n",
        "#     epsilon = 0.99\n",
        "#     decay_factor = 0.9999\n",
        "#     Q = np.random.rand(env.get_state_size(), env.get_action_size())\n",
        "#     policy =self.get_e_soft_greedy_policy(env,epsilon,np.zeros((env.get_state_size(), env.get_action_size())))\n",
        "#     V = np.zeros(env.get_state_size())\n",
        "#     total_rewards = []\n",
        "#     values = [V]\n",
        "#     state_size = env.get_state_size()\n",
        "#     action_size = env.get_action_size()\n",
        "#     gamma=env.get_gamma()\n",
        "#     Returns = [[[] for _ in range(action_size)] for _ in range(state_size)]\n",
        "    \n",
        "#     for episode in range(episodes_no):\n",
        "#       # reset\n",
        "#       reward_sum = 0\n",
        "#       t, state, reward, done = env.reset()\n",
        "#       # sample trace\n",
        "#       trace_states = []\n",
        "#       trace_rewards = []\n",
        "#       trace_actions = []\n",
        "#       G = 0\n",
        "#       while done!=1:\n",
        "#         best_action = np.random.choice(action_size, p = policy[state, :])\n",
        "#         _, next_state, reward, done = env.step(best_action)\n",
        "#         trace_states.append(state)\n",
        "#         trace_rewards.append(reward)\n",
        "#         trace_actions.append(best_action)\n",
        "#         reward_sum+=reward\n",
        "#         state=next_state\n",
        "#       for time_step in reversed(range(0,len(trace_states))):\n",
        "#         G = gamma*G + trace_rewards[time_step]\n",
        "#         #if not any((s == state and a == action) for s, a, _ in trace[:time_step]):\n",
        "#         Returns[trace_states[time_step]][trace_actions[time_step]].append(G)\n",
        "#         Q[trace_states[time_step],trace_actions[time_step]] = np.mean(Returns[trace_states[time_step]][trace_actions[time_step]])\n",
        "#         best_action = np.argmax(Q[trace_states[time_step], :])\n",
        "#         for action in range(action_size):\n",
        "#           if action==best_action:\n",
        "#             policy[trace_states[time_step],action] = 1-epsilon+epsilon/action_size\n",
        "#           else:\n",
        "#             policy[trace_states[time_step],action] = epsilon/action_size\n",
        "\n",
        "#       V = np.max(Q,axis=1)\n",
        "#       values.append(V)\n",
        "#       total_rewards.append(reward_sum)\n",
        "#       epsilon = epsilon*decay_factor\n",
        "    \n",
        "\n",
        "\n",
        "#     return policy, values, total_rewards\n",
        "\n",
        "#   def plot_learning_curve(self,env,no_trials,no_episodes):\n",
        "#       rewards_list=[]\n",
        "#       for trial in range(1,no_trials+1):\n",
        "#         print(\"run \"+ str(trial))\n",
        "#         _,_,trial_reward = self.solve(env)\n",
        "#         rewards_list.append(trial_reward)\n",
        "#       #episodes = [episode for episode in no_episodes]\n",
        "#       rewards_array = np.array(rewards_list)\n",
        "#       mean_rewards = np.mean(rewards_array,axis=0)\n",
        "#       std_rewards = np.std(rewards_array,axis=0)\n",
        "#       plt.plot(range(1,len(mean_rewards)+1),mean_rewards, label='Mean', color='blue')\n",
        "#       plt.fill_between(range(1,len(mean_rewards)+1), mean_rewards+std_rewards,mean_rewards-std_rewards,alpha=0.5)\n",
        "#       plt.title(\"Mean and standard deviation of total reward across 25 trials\")\n",
        "#       legend_el = [plt.Line2D([0], [0], color='b', label='Mean'),\n",
        "#                       plt.Rectangle((0, 0), 1, 1, fc=\"skyblue\", alpha=0.5, edgecolor='none', label='Standard Deviation')]\n",
        "#       # Add labels and a legend\n",
        "#       plt.xlabel('Episode number')\n",
        "#       plt.ylabel('Total reward')\n",
        "#       plt.legend(handles=legend_el, loc = 'lower right')\n",
        "\n",
        "#       # Show the plot\n",
        "#       plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMkZKrh6VUgw"
      },
      "source": [
        "## TD agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1576,
      "metadata": {
        "id": "_Xyko9SvrGbE"
      },
      "outputs": [],
      "source": [
        "# This class define the Temporal-Difference agent\n",
        "\n",
        "class TD_agent(object):\n",
        "\n",
        "  def learning_curve(self, env):\n",
        "    plt.figure(figsize=(15,10))\n",
        "    # Constant epsilon = 0.1\n",
        "    epsilon = 0.3\n",
        "    alphas = [0.01, 0.04, 0.07, 0.1, 0.4, 0.7]\n",
        "    for alpha in alphas:\n",
        "      rewards_per_alpha = []\n",
        "      for i in range(50):\n",
        "        _, _, total_rewards = self.graph_solve(env, epsilon, alpha)\n",
        "        rewards_per_alpha.append(total_rewards)\n",
        "      rewards_per_alpha = np.array(rewards_per_alpha)\n",
        "      mean_rewards = np.mean(rewards_per_alpha, axis = 0)\n",
        "      plt.plot(mean_rewards, label = '\\u03B1 = ' + str(alpha))\n",
        "      print('alpha', alpha, 'last mean reward', rewards_per_alpha[:,-1])\n",
        "    plt.title('Learning Curve for TD Agent for varying \\u03B1 values and constant \\u03B5 = 0.1', fontsize = 16)\n",
        "    plt.xlabel('Episode Number', fontsize = 14)\n",
        "    plt.ylabel('Total Reward', fontsize = 14)\n",
        "    plt.legend(fontsize = 12, loc = 'lower right')\n",
        "    plt.grid(alpha = 0.3)\n",
        "    plt.show()\n",
        "    # Constant alpha = 0.05\n",
        "    plt.figure(figsize=(15,10))\n",
        "    epsilons = [0, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
        "    alpha = 0.05\n",
        "    for epsilon in epsilons:\n",
        "      rewards_per_epsilon = []\n",
        "      for i in range(50):\n",
        "        _, _, total_rewards = self.graph_solve(env, epsilon, alpha)\n",
        "        rewards_per_epsilon.append(total_rewards)\n",
        "      rewards_per_epsilon = np.array(rewards_per_epsilon)\n",
        "      mean_rewards = np.mean(rewards_per_epsilon, axis = 0)\n",
        "      plt.plot(mean_rewards, label = '\\u03B5 = ' + str(epsilon))\n",
        "      print('epsilon', epsilon, 'last mean reward', rewards_per_epsilon[:,-1])\n",
        "    plt.title('Learning Curve for TD Agent for varying \\u03B5 values and constant \\u03B1 = 0.05', fontsize = 16)\n",
        "    plt.xlabel('Episode Number', fontsize = 14)\n",
        "    plt.ylabel('Total Reward', fontsize = 14)\n",
        "    plt.legend(fontsize = 12, loc = 'lower right')\n",
        "    plt.grid(alpha = 0.3)\n",
        "    plt.show()\n",
        "    \n",
        "  \n",
        "  # [Action required]\n",
        "  # WARNING: make sure this function can be called by the auto-marking script\n",
        "  def solve(self, env):\n",
        "    \"\"\"\n",
        "    Solve a given Maze environment using Temporal Difference learning\n",
        "    input: env {Maze object} -- Maze to solve\n",
        "    output: \n",
        "      - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
        "      - values {list of np.array} -- List of successive value functions for each episode \n",
        "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode \n",
        "    \"\"\"\n",
        "\n",
        "    # Initialisation (can be edited)\n",
        "    Q = np.random.rand(env.get_state_size(), env.get_action_size()) \n",
        "    V = np.zeros(env.get_state_size())\n",
        "    policy = np.zeros((env.get_state_size(), env.get_action_size())) \n",
        "    values = [V]\n",
        "    total_rewards = []\n",
        "    gamma = env.get_gamma()\n",
        "    epsilon = 0.99\n",
        "\n",
        "    #### \n",
        "    # Add your code here\n",
        "    # WARNING: this agent only has access to env.reset() and env.step()\n",
        "    # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value\n",
        "    ####\n",
        "\n",
        "    # Q-Learning\n",
        "    for episode_count in range(5000):\n",
        "      # Update alpha\n",
        "      alpha = (episode_count + 1) ** (-0.51)\n",
        "      # Reset environment\n",
        "      t, state, reward, done = env.reset()\n",
        "      total_reward = 0\n",
        "      # Generate episode\n",
        "      step_count = 0\n",
        "      while not done and step_count < 500:\n",
        "        step_count += 1\n",
        "        # Make an epsilon-greedy policy from Q and Choose action\n",
        "        best_action = np.argmax(Q[state, :])\n",
        "        for action in range(env.get_action_size()):\n",
        "          if action == best_action:\n",
        "            policy[state, action] = 1 - epsilon + epsilon / env.get_action_size()\n",
        "          else:\n",
        "            policy[state, action] = epsilon / env.get_action_size()\n",
        "        action = np.random.choice(env.get_action_size(), p = policy[state, :])\n",
        "        # Perform action\n",
        "        t, next_state, reward, done = env.step(action)\n",
        "        # Update Q\n",
        "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
        "        # Update state and total reward\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "      # Compute total reward\n",
        "      total_rewards.append(total_reward)\n",
        "      # Update values\n",
        "      V = np.max(Q, axis = 1)\n",
        "      values.append(V)\n",
        "      # Update epsilon\n",
        "      epsilon = epsilon * (0.9999995 ** episode_count)\n",
        "      \n",
        "      print('episode', episode_count, 'epsilon', epsilon, 'alpha', alpha)\n",
        "    # Compute optimal policy\n",
        "    for state in range(env.get_state_size()):\n",
        "      policy[state, np.argmax(Q[state, :])] = 1\n",
        "    \n",
        "    return policy, values, total_rewards\n",
        "  \n",
        "\n",
        "  def graph_solve(self, env, epsilon, alpha):\n",
        "    \"\"\"\n",
        "    Solve a given Maze environment using Temporal Difference learning\n",
        "    input: env {Maze object} -- Maze to solve\n",
        "    output: \n",
        "      - policy {np.array} -- Optimal policy found to solve the given Maze environment \n",
        "      - values {list of np.array} -- List of successive value functions for each episode \n",
        "      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode \n",
        "    \"\"\"\n",
        "\n",
        "    # Initialisation (can be edited)\n",
        "    Q = np.random.rand(env.get_state_size(), env.get_action_size()) \n",
        "    V = np.zeros(env.get_state_size())\n",
        "    policy = np.zeros((env.get_state_size(), env.get_action_size())) \n",
        "    values = [V]\n",
        "    total_rewards = []\n",
        "    gamma = env.get_gamma()\n",
        "\n",
        "    #### \n",
        "    # Add your code here\n",
        "    # WARNING: this agent only has access to env.reset() and env.step()\n",
        "    # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value\n",
        "    ####\n",
        "\n",
        "    # Q-Learning\n",
        "    for episode_count in range(200):\n",
        "      # Reset environment\n",
        "      t, state, reward, done = env.reset()\n",
        "      total_reward = 0\n",
        "      # Generate episode\n",
        "      step_count = 0\n",
        "      while not done and step_count < 500:\n",
        "        step_count += 1\n",
        "        # Make an epsilon-greedy policy from Q and Choose action\n",
        "        best_action = np.argmax(Q[state, :])\n",
        "        for action in range(env.get_action_size()):\n",
        "          if action == best_action:\n",
        "            policy[state, action] = 1 - epsilon + epsilon / env.get_action_size()\n",
        "          else:\n",
        "            policy[state, action] = epsilon / env.get_action_size()\n",
        "        action = np.random.choice(env.get_action_size(), p = policy[state, :])\n",
        "        # Perform action\n",
        "        t, next_state, reward, done = env.step(action)\n",
        "        # Update Q\n",
        "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
        "        # Update state and total reward\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "      # Compute total reward\n",
        "      total_rewards.append(total_reward)\n",
        "      # Update values\n",
        "      V = np.max(Q, axis = 1)\n",
        "      values.append(V)\n",
        "\n",
        "    # Compute optimal policy\n",
        "    for state in range(env.get_state_size()):\n",
        "      policy[state, np.argmax(Q[state, :])] = 1\n",
        "    \n",
        "    return policy, values, total_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzSzRSO6VWVD"
      },
      "source": [
        "## Example main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1579,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eyeJfvwXp3ta",
        "outputId": "229af227-7973-4819-dc05-ff8219987805",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating the Maze:\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Nabee\\rl\\coursework_1_maze.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# maze = Maze()\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# ### Question 1: Dynamic programming\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m### Question 2: Monte-Carlo learning\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m mc_agent \u001b[39m=\u001b[39m MC_agent()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m mc_policy, mc_values, total_rewards \u001b[39m=\u001b[39m mc_agent\u001b[39m.\u001b[39;49msolve(maze)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mResults of the MC agent:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m maze\u001b[39m.\u001b[39mget_graphics()\u001b[39m.\u001b[39mdraw_policy(mc_policy)\n",
            "\u001b[1;32mc:\\Users\\Nabee\\rl\\coursework_1_maze.ipynb Cell 17\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m   policy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_e_soft_greedy_policy(env,epsilon,Q,state,policy)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m#for state in range(state_size):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m#   V[state] = np.max(Q[state,:])\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m V \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmax(Q,axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m values\u001b[39m.\u001b[39mappend(V)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nabee/rl/coursework_1_maze.ipynb#X20sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m total_rewards\u001b[39m.\u001b[39mappend(reward_sum)\n",
            "File \u001b[1;32mc:\\Users\\Nabee\\rl\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2693\u001b[0m \u001b[39m@set_module\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   2694\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[0;32m   2695\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[0;32m   2696\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2697\u001b[0m \u001b[39m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[39m    5\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmaximum, \u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[0;32m   2811\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
            "File \u001b[1;32mc:\\Users\\Nabee\\rl\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Example main (can be edited)\n",
        "\n",
        "### Question 0: Defining the environment\n",
        "\n",
        "print(\"Creating the Maze:\\n\")\n",
        "# maze = Maze()\n",
        "\n",
        "# ### Question 1: Dynamic programming\n",
        "# dp_agent = DP_agent()\n",
        "# dp_policy, dp_value = dp_agent.solve(maze)\n",
        "\n",
        "# print(\"Results of the DP agent:\\n\")\n",
        "# maze.get_graphics().draw_policy(dp_policy)\n",
        "# maze.get_graphics().draw_value(dp_value)\n",
        "\n",
        "\n",
        "### Question 2: Monte-Carlo learning\n",
        "\n",
        "mc_agent = MC_agent()\n",
        "mc_policy, mc_values, total_rewards = mc_agent.solve(maze)\n",
        "\n",
        "print(\"Results of the MC agent:\\n\")\n",
        "maze.get_graphics().draw_policy(mc_policy)\n",
        "maze.get_graphics().draw_value(mc_values[-1])\n",
        "\n",
        "# mc_agent.learning_curve(maze)\n",
        "\n",
        "\n",
        "# ## Question 3: Temporal-Difference learning\n",
        "\n",
        "# td_agent = TD_agent()\n",
        "# td_policy, td_values, total_rewards = td_agent.solve(maze)\n",
        "\n",
        "# print(\"Results of the TD agent:\\n\")\n",
        "# maze.get_graphics().draw_policy(td_policy)\n",
        "# maze.get_graphics().draw_value(td_values[-1])\n",
        "\n",
        "# td_agent.learning_curve(maze)\n",
        "\n",
        "# maze = Maze()\n",
        "# mc_agent = MC_agent()\n",
        "# mc_agent.plot_learning_curve(maze,25,5000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "lbY8DCqoVJlw",
        "DW3Ul0q-VRE-",
        "14i0zRkdVSqk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
